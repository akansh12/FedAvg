{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7d91d49fb910>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import numpy as np\n",
    "import torchvision\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple cnn for mnist\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(1600, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 1600)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple training function\n",
    "def train_epoch(model, dataloader, optimizer, loss_function, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_correct = 0\n",
    "    for data, target in dataloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        train_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    accuracy = train_correct / len(dataloader.dataset)\n",
    "    return model, total_loss, accuracy\n",
    "\n",
    "#simple testing function\n",
    "def test_model(model, dataloader, loss_function, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    test_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += loss_function(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            test_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    accuracy = test_correct / len(dataloader.dataset)\n",
    "    return total_loss, accuracy\n",
    "\n",
    "def train(model, train_loader, test_loader, optimizer, loss_function, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model, train_loss, train_accuracy = train_epoch(model, train_loader, optimizer, loss_function, device)\n",
    "        if test_loader:\n",
    "            test_loss, test_accuracy = test_model(model, test_loader, loss_function, device)\n",
    "            # print(f'Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss:.4f} - Train Accuracy: {train_accuracy:.4f} - Test Loss: {test_loss:.4f} - Test Accuracy: {test_accuracy:.4f}')\n",
    "        else:\n",
    "            # print(f'Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss:.4f} - Train Accuracy: {train_accuracy:.4f}')\n",
    "            test_accuracy = 'nan'\n",
    "            test_loss = 'nan'\n",
    "    return model, train_loss, train_accuracy, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/cifar-10-python.tar.gz to ../data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#load CIFAR10 dataset\n",
    "model = torchvision.models.resnet18(weights=None)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "# model = SimpleCNN().to(\"cuda\")\n",
    "model = torchvision.models.resnet18(weights=None).to(\"cuda\")\n",
    "model.fc = nn.Linear(512, 10).to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "model, train_loss, train_acc, test_loss, test_acc = train(model, train_loader, test_loader, optimizer, loss_function, \"cuda\", num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config):\n",
    "    if config[\"model\"] == \"SimpleCNN\":\n",
    "        return SimpleCNN()\n",
    "    elif config[\"model\"] == \"ResNet18\":\n",
    "        model = torchvision.models.resnet18(weights=None)\n",
    "        model.fc = nn.Linear(512, 10)\n",
    "        return model\n",
    "    else:\n",
    "        raise ValueError(\"Model not supported\")\n",
    "    \n",
    "def load_data(config):\n",
    "    if config[\"dataset\"] == \"MNIST\":\n",
    "        data = datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5), (0.5))]))\n",
    "    elif config[\"dataset\"] == \"CIFAR10\":\n",
    "        data = datasets.CIFAR10('../data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5), (0.5))]))\n",
    "    else:\n",
    "        raise ValueError(\"Dataset not supported\")\n",
    "    \n",
    "    if config[\"iid\"]:\n",
    "        client_data = random_split(data, [len(data) // config['num_clients'] for _ in range(config['num_clients'])])\n",
    "    else:\n",
    "        client_data = create_noniid_dataset_mnist(data, config['num_clients'], config['num_labels_per_client'])\n",
    "    return client_data\n",
    "\n",
    "def get_testloader(config):\n",
    "    if config[\"dataset\"] == \"MNIST\":\n",
    "        data = datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5), (0.5))]))\n",
    "    elif config[\"dataset\"] == \"CIFAR10\":\n",
    "        data = datasets.CIFAR10('../data', train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5), (0.5))]))\n",
    "    else:\n",
    "        raise ValueError(\"Dataset not supported\")\n",
    "    return torch.utils.data.DataLoader(data, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "\n",
    "def create_noniid_dataset_mnist(data, num_clients, num_labels_per_client):\n",
    "    \"\"\"\n",
    "    Splits the dataset into non-IID subsets, where each client gets data for a fixed number of labels.\n",
    "    \n",
    "    Args:\n",
    "        data: The dataset to be split.\n",
    "        num_clients: The number of clients.\n",
    "        num_labels_per_client: Number of unique labels per client.\n",
    "    \n",
    "    Returns:\n",
    "        A list of Subset objects, one for each client.\n",
    "    \"\"\"\n",
    "    # Group indices by labels\n",
    "    label_indices = {i: [] for i in range(10)} \n",
    "    for idx, (_, label) in enumerate(data):\n",
    "        label_indices[label].append(idx)\n",
    "    \n",
    "    # Shuffle indices within each label\n",
    "    for label in label_indices:\n",
    "        torch.manual_seed(42)  # For reproducibility\n",
    "        torch.randperm(len(label_indices[label])).tolist()\n",
    "    \n",
    "    # Assign labels to clients\n",
    "    clients = [[] for _ in range(num_clients)]\n",
    "    label_list = list(label_indices.keys())\n",
    "    \n",
    "    # Divide labels across clients\n",
    "    labels_per_client = len(label_list) // num_clients\n",
    "    for client_id in range(num_clients):\n",
    "        start_label = (client_id * labels_per_client) % len(label_list)\n",
    "        client_labels = label_list[start_label:start_label + num_labels_per_client]\n",
    "        \n",
    "        # Add data for these labels to the client\n",
    "        for label in client_labels:\n",
    "            clients[client_id].extend(label_indices[label])\n",
    "    \n",
    "    # Create Subset objects for each client\n",
    "    client_data = [Subset(data, client_indices) for client_indices in clients]\n",
    "    return client_data\n",
    "\n",
    "\n",
    "class FedAVG():\n",
    "    def __init__(self, config) -> None:\n",
    "        self.config = config\n",
    "        self.global_model = get_model(config).to(config[\"device\"])\n",
    "        self.client_data = load_data(config)\n",
    "        self.testloader = get_testloader(config)\n",
    "\n",
    "    def client_train(self, client_id):\n",
    "        data = self.client_data[client_id]\n",
    "        model = get_model(self.config).to(self.config[\"device\"])\n",
    "        model.load_state_dict(self.global_model.state_dict())\n",
    "        optimizer = getattr(optim, self.config[\"optimizer\"])(model.parameters(), lr=self.config[\"lr\"])\n",
    "        loss_function = getattr(nn, self.config[\"loss_function\"])()\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(data, batch_size=self.config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "        model, local_train_loss, local_train_accuracy, _, _ = train(model, data_loader, None, optimizer, loss_function, self.config[\"device\"], self.config[\"num_epochs\"])\n",
    "\n",
    "        return model.state_dict()\n",
    "    \n",
    "    def aggregate(self, weights, client_sample_sizes):\n",
    "        total_samples = np.sum(client_sample_sizes)\n",
    "        avg_weights = {}\n",
    "        for key in weights[0].keys():\n",
    "            avg_weights[key] = sum(\n",
    "                weights[i][key] * (client_sample_sizes[i] / total_samples)\n",
    "                for i in range(len(weights)))\n",
    "        return avg_weights\n",
    "    \n",
    "    def run(self):\n",
    "        for round in range(self.config[\"num_rounds\"]):\n",
    "            client_weights = []\n",
    "            client_sample_sizes = []\n",
    "            selected_clients = np.random.choice(len(self.client_data), self.config[\"num_client_selection\"], replace=False)\n",
    "            for client_id in selected_clients:\n",
    "                client_weights.append(self.client_train(client_id))\n",
    "                client_sample_sizes.append(len(self.client_data[client_id]))\n",
    "            aggregated_weights = self.aggregate(client_weights, client_sample_sizes)\n",
    "            self.global_model.load_state_dict(aggregated_weights)\n",
    "            print(f\"Round {round + 1} completed\")\n",
    "\n",
    "            test_loss, test_accuracy = test_model(self.global_model,self.testloader, getattr(nn, self.config[\"loss_function\"])(), self.config[\"device\"])\n",
    "            print(f\"Test Loss: {test_loss:.4f} - Test Accuracy: {test_accuracy:.4f}\")\n",
    "            print(\"-\"*50)\n",
    "        return self.global_model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\": \"ResNet18\",\n",
    "    \"dataset\": \"CIFAR10\",\n",
    "    \"iid\": True,\n",
    "    \"num_clients\": 10,\n",
    "    \"num_epochs\": 2,\n",
    "    \"lr\": 0.01,\n",
    "    \"num_rounds\": 10,\n",
    "    \"device\": 'cuda',\n",
    "    \"batch_size\": 32,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"loss_function\": \"CrossEntropyLoss\",\n",
    "    \"num_client_selection\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "fl_avg = FedAVG(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 completed\n",
      "Test Loss: 721.2015 - Test Accuracy: 0.1000\n",
      "--------------------------------------------------\n",
      "Round 2 completed\n",
      "Test Loss: 720.0008 - Test Accuracy: 0.1000\n",
      "--------------------------------------------------\n",
      "Round 3 completed\n",
      "Test Loss: 606.1033 - Test Accuracy: 0.2617\n",
      "--------------------------------------------------\n",
      "Round 4 completed\n",
      "Test Loss: 460.6566 - Test Accuracy: 0.4750\n",
      "--------------------------------------------------\n",
      "Round 5 completed\n",
      "Test Loss: 406.6692 - Test Accuracy: 0.5335\n",
      "--------------------------------------------------\n",
      "Round 6 completed\n",
      "Test Loss: 371.5809 - Test Accuracy: 0.5701\n",
      "--------------------------------------------------\n",
      "Round 7 completed\n",
      "Test Loss: 347.8602 - Test Accuracy: 0.5971\n",
      "--------------------------------------------------\n",
      "Round 8 completed\n",
      "Test Loss: 320.6075 - Test Accuracy: 0.6375\n",
      "--------------------------------------------------\n",
      "Round 9 completed\n",
      "Test Loss: 298.4674 - Test Accuracy: 0.6664\n",
      "--------------------------------------------------\n",
      "Round 10 completed\n",
      "Test Loss: 286.7061 - Test Accuracy: 0.6843\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "global_model = fl_avg.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\": \"SimpleCNN\",\n",
    "    \"dataset\": \"MNIST\",\n",
    "    \"iid\": False,\n",
    "    \"num_clients\": 10,\n",
    "    \"num_epochs\": 2,\n",
    "    \"lr\": 0.01,\n",
    "    \"num_rounds\": 10,\n",
    "    \"device\": 'cuda',\n",
    "    \"batch_size\": 32,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"loss_function\": \"CrossEntropyLoss\",\n",
    "    \"num_client_selection\": 6,\n",
    "    \"num_labels_per_client\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_avg = FedAVG(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 completed\n",
      "Test Loss: 718.0779 - Test Accuracy: 0.1698\n",
      "--------------------------------------------------\n",
      "Round 2 completed\n",
      "Test Loss: 631.9236 - Test Accuracy: 0.2781\n",
      "--------------------------------------------------\n",
      "Round 3 completed\n",
      "Test Loss: 672.9356 - Test Accuracy: 0.3972\n",
      "--------------------------------------------------\n",
      "Round 4 completed\n",
      "Test Loss: 555.6201 - Test Accuracy: 0.4403\n",
      "--------------------------------------------------\n",
      "Round 5 completed\n",
      "Test Loss: 565.7080 - Test Accuracy: 0.5105\n",
      "--------------------------------------------------\n",
      "Round 6 completed\n",
      "Test Loss: 546.7292 - Test Accuracy: 0.4844\n",
      "--------------------------------------------------\n",
      "Round 7 completed\n",
      "Test Loss: 437.6212 - Test Accuracy: 0.6330\n",
      "--------------------------------------------------\n",
      "Round 8 completed\n",
      "Test Loss: 401.4468 - Test Accuracy: 0.6732\n",
      "--------------------------------------------------\n",
      "Round 9 completed\n",
      "Test Loss: 311.9458 - Test Accuracy: 0.7171\n",
      "--------------------------------------------------\n",
      "Round 10 completed\n",
      "Test Loss: 338.0167 - Test Accuracy: 0.6766\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "global_model = fl_avg.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
